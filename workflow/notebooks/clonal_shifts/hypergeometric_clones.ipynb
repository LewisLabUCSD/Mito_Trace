{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ae72f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Input info\n",
    "se_cells_meta_f = \"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/gff_A2_black/annotation_clones/se_cells_meta_labels.tsv\"\n",
    "outdir = \"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/clonal_shifts/clones/\"\n",
    "\n",
    "clone_col = \"name\"\n",
    "atac_col = \"cluster_labels\"\n",
    "\n",
    "# config\n",
    "N_DONORS = 2\n",
    "input_cond = \"Input\"\n",
    "condition = \"inputOnly\" # noInput\n",
    "\n",
    "# params\n",
    "min_clone_size = 10\n",
    "p_thresh = 0.1 \n",
    "\n",
    "n_cpus = 12\n",
    "\n",
    "n_shuffle=1000\n",
    "#conds_sep = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join, exists\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.stats import hypergeom, fisher_exact\n",
    "from statsmodels.stats import multitest \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import clonal_shifts as cs\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62868ce3",
   "metadata": {},
   "source": [
    "### Setup outdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ad9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = join(outdir,\"combDonors\")\n",
    "sep_dir = join(outdir,\"sepDonors\")\n",
    "\n",
    "\n",
    "if not exists(all_dir):\n",
    "    os.mkdir(all_dir)\n",
    "    \n",
    "if not exists(sep_dir):\n",
    "    os.mkdir(sep_dir)\n",
    "\n",
    "donor_out = {}\n",
    "for d in np.arange(N_DONORS):\n",
    "    donor_out[d] = join(sep_dir, f\"donor{d}\")\n",
    "    if not exists(donor_out[d]):\n",
    "        os.mkdir(donor_out[d])\n",
    "        \n",
    "    \n",
    "# sim_all_dir = join(all_dir, \"shuffle\")\n",
    "# sim_sep_dir = join(sep_dir, \"shuffle\")\n",
    "# if not exists(sep_dir):\n",
    "#     os.mkdir(sep_dir)\n",
    "# if not exists(sim_all_dir):\n",
    "#     os.mkdir(sim_all_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727ea54",
   "metadata": {},
   "source": [
    "## Load barcodes, and add donor id for the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24101f7",
   "metadata": {},
   "source": [
    "## Load cells_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd63a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cells_meta = pd.read_csv(se_cells_meta_f, sep=\"\\t\")\n",
    "cells_meta = cells_meta.loc[~(cells_meta[\"name\"]==\"None\")]\n",
    "\n",
    "if not \"cluster_labels\" in cells_meta.columns.values:\n",
    "    cells_meta[\"cluster_labels\"] = cells_meta[\"seurat_clusters\"]\n",
    "cells_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de57f2",
   "metadata": {},
   "source": [
    "## Map the new group to cells_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aae783",
   "metadata": {},
   "source": [
    "## Filter for condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60d05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if condition == \"inputOnly\":\n",
    "    cells_meta = cells_meta.loc[cells_meta[\"condition\"]==input_cond]\n",
    "else:\n",
    "    cells_meta = cells_meta.loc[cells_meta[\"condition\"]!=input_cond]\n",
    "#sns.countplot(data=cells_meta,x=clone_col)\n",
    "cells_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaeb86",
   "metadata": {},
   "source": [
    "## construct the clone_groups and atac_cl_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef726ad8",
   "metadata": {},
   "source": [
    "## A) Loop through donor and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0081680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d, val in cells_meta.groupby(\"donor\"):\n",
    "    print('donor', d)\n",
    "    curr_groups = val.groupby([atac_col, clone_col]).size().reset_index().rename({0:\"count\"}, axis=1)\n",
    "    curr_groups\n",
    "    curr_sizes = val.groupby(clone_col).size().sort_values(ascending=False)\n",
    "    curr_name_cond_size = val.groupby([clone_col]).size()\n",
    "    curr_name_cond_size = curr_name_cond_size[curr_name_cond_size>min_clone_size]\n",
    "    curr_clones_filt = curr_name_cond_size.index \n",
    "    curr_sizes = curr_sizes.loc[curr_clones_filt].sort_values(ascending=False)\n",
    "    \n",
    "    curr_clones = np.unique(curr_groups[clone_col])\n",
    "    curr_clone_map = {x:ind for ind,x in enumerate(curr_clones)}\n",
    "    print(curr_clones)\n",
    "    curr_atac_cl = np.unique(curr_groups[atac_col])\n",
    "    print(curr_atac_cl)\n",
    "\n",
    "    curr_don_out = donor_out[int(d)]\n",
    "    \n",
    "    # Run init hypergeo and the counts data\n",
    "    cs.hypergeo_plots(curr_groups, curr_clones, curr_atac_cl, curr_sizes, p_thresh, atac_col,\n",
    "                   clone_col, curr_don_out)\n",
    "    \n",
    "    out_df, hyper_df, results_df, out_d = cs.run_data_and_shuffle(curr_groups, curr_don_out, atac_col, clone_col, p_thresh, curr_clones, \n",
    "                                                      curr_atac_cl, n_shuffle=n_shuffle, figs_close=False, to_p_correct=False,\n",
    "                                                                  n_cpus=n_cpus)\n",
    "\n",
    "#     print(\"plotting counts\")\n",
    "#     curr_groups[\"log2_count\"] = np.log2(curr_groups[\"count\"]+1)\n",
    "#     g = sns.clustermap(curr_groups.pivot(index=atac_col,columns=clone_col, values=\"log2_count\").fillna(0))\n",
    "#     plt.gca().set_title(\"log2 ncells\")\n",
    "#     plt.savefig(join(curr_don_out, \"ncells.png\"))\n",
    "#     curr_groups = curr_groups.drop(\"log2_count\", axis=1)\n",
    "    \n",
    "#     output_df, bh_enrichment_df = cs.pipeline_groups_hypergeo(curr_groups, curr_clones, curr_atac_cl, curr_sizes,\n",
    "#                                                              p_thresh=p_thresh, atac_col=atac_col, \n",
    "#                                                               clone_col=clone_col)\n",
    "\n",
    "    \n",
    "#     bh_enrichment_df.to_csv(join(curr_don_out, \"hypergeo_input_padjusted.csv\"))\n",
    "#     output_df.to_csv(join(curr_don_out, \"hypergeo_input_padjusted_sigOnly.csv\"))\n",
    "    \n",
    "#     if output_df.shape[0] == 0:   \n",
    "#         g = sns.clustermap(-np.log10(bh_enrichment_df.fillna(1)), \n",
    "#                        row_cluster=False)\n",
    "#         g.fig.suptitle(\"No groups were significant\")\n",
    "#     else:\n",
    "#         g = sns.clustermap(-np.log10(bh_enrichment_df.loc[output_df.index].fillna(1)), \n",
    "#                        row_cluster=False)\n",
    "#     g.ax_heatmap.set(xlabel=\"Cluster ID\")\n",
    "#     g.ax_cbar.set(title=\"-log10 p-value\")\n",
    "#     g.fig.suptitle(f\"Hypergeometric distribution p-values - non-significant values (p={p_thresh}) are zeroes\")\n",
    "#     plt.savefig(join(curr_don_out, \"hypergeo_input_padjusted_sigOnly.png\"))\n",
    "    \n",
    "#     init_bh_enrichment = cs.create_enrichment(curr_groups, atac_col, clone_col, p_thresh,\n",
    "#                                               clones=curr_clones, atac_cl=curr_atac_cl)\n",
    "    \n",
    "#     shuffle = cs.shuffle_hypergeo(curr_groups, atac_col, clone_col, p_thresh, curr_clones, curr_atac_cl, \n",
    "#                                   n_shuffle=n_shuffle, to_parallel=True, n_cpus=n_cpus)\n",
    "\n",
    "#     results_df, out_d = cs.get_out(shuffle, curr_clones, init_bh_enrichment, p_thresh, \n",
    "#                                                           curr_clone_map, atac_col, \n",
    "#                                                           outdir=join(curr_don_out, \"shuffle\"))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443742e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571b7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f9ada",
   "metadata": {},
   "source": [
    "## B) Run using all donors as background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf863bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sizes = cells_meta.groupby(clone_col).size().sort_values(ascending=False)\n",
    "name_cond_size = cells_meta.groupby([clone_col]).size()\n",
    "name_cond_size = name_cond_size[name_cond_size>min_clone_size]\n",
    "clones_filt = name_cond_size.index \n",
    "\n",
    "sizes = sizes.loc[clones_filt].sort_values(ascending=False)\n",
    "clones_filt\n",
    "\n",
    "groups = cells_meta.groupby([atac_col, clone_col]).size().reset_index().rename({0:\"count\"}, axis=1)\n",
    "groups\n",
    "\n",
    "clones = clones_filt.values #np.unique(groups[\"name\"])\n",
    "clone_map = {x:ind for ind,x in enumerate(clones)}\n",
    "\n",
    "atac_cl = np.unique(groups[atac_col])\n",
    "atac_cl\n",
    "\n",
    "\n",
    "# Run init hypergeo\n",
    "cs.hypergeo_plots(groups, clones, atac_cl, sizes, p_thresh, atac_col,\n",
    "               clone_col, all_dir)\n",
    "## Run shuffle\n",
    "print(\"Running hypergeo shuffle and saving sig results\")\n",
    "out_df, hyper_df, results_df, out_d = cs.run_data_and_shuffle(groups, all_dir, atac_col, clone_col, p_thresh, clones, \n",
    "                                                  atac_cl, n_shuffle=n_shuffle, figs_close=False, to_p_correct=False,\n",
    "                                                              n_cpus=n_cpus)\n",
    "\n",
    "# # plot just the counts\n",
    "# print(\"plotting counts\")\n",
    "# groups[\"log2_count\"] = np.log2(groups[\"count\"]+1)\n",
    "# g = sns.clustermap(groups.pivot(index=atac_col,columns=clone_col, values=\"log2_count\").fillna(0))\n",
    "# plt.gca().set_title(\"log2 ncells\")\n",
    "# plt.savefig(join(all_dir, \"ncells.png\"))\n",
    "\n",
    "\n",
    "# print(\"Running hypergeo and saving sig results\")\n",
    "# output_df, bh_enrichment_df = cs.pipeline_groups_hypergeo(groups, clones, atac_cl, sizes,p_thresh, atac_col, clone_col)\n",
    "# bh_enrichment_df.to_csv(join(all_dir, \"hypergeo_padjusted.csv\"))\n",
    "# output_df.to_csv(join(all_dir, \"hypergeo_padjusted_sigOnly.csv\"))\n",
    "    \n",
    "# if output_df.shape[0] == 0:   \n",
    "#     g = sns.heatmap(-np.log10(bh_enrichment_df.fillna(1)))\n",
    "#     g.fig.suptitle(\"No groups were significant\")\n",
    "# else:\n",
    "#     g = sns.clustermap(-np.log10(bh_enrichment_df.loc[output_df.index].fillna(1)), \n",
    "#                    row_cluster=False)\n",
    "#     g.ax_heatmap.set(xlabel=\"Cluster ID\")\n",
    "#     g.ax_cbar.set(title=\"-log10 p-value\")\n",
    "# plt.savefig(join(all_dir, \"hypergeo_padjusted_sigOnly.png\"))\n",
    "\n",
    "# init_bh_enrichment = cs.create_enrichment(groups, atac_col, clone_col, p_thresh,\n",
    "#                                           clones=clones, atac_cl=atac_cl)\n",
    "\n",
    "# shuffle = cs.shuffle_hypergeo(groups, atac_col, clone_col, p_thresh, clones, atac_cl, n_shuffle=n_shuffle, \n",
    "#                               to_parallel=True, n_cpus=24)\n",
    "\n",
    "# results_df, out_d = cs.get_out(shuffle, clones, init_bh_enrichment_df, p_thresh, clone_map, atac_col, \n",
    "#                                                       all_dir=sim_all_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f1c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_df\n",
    "\n",
    "# ## Plot shuffle p-value heatmap for each method \n",
    "\n",
    "# for ind, val in results_df[results_df[\"value\"]<p_thresh].groupby(\"method\"):\n",
    "#     sns.clustermap(val.astype(object).pivot(index=\"index\", columns=\"variable\",values=\"value\").fillna(1),\n",
    "#                   vmax=p_thresh+0.1)\n",
    "#     plt.suptitle(ind)\n",
    "\n",
    "# ## Same but log10\n",
    "\n",
    "# for ind, val in results_df[results_df[\"value\"]<p_thresh].groupby(\"method\"):\n",
    "#     min_m = min(val.loc[val[\"value\"] != 0 , \"value\"])\n",
    "#     val.loc[val[\"value\"] == 0 ,\"value\"] = min_m\n",
    "    \n",
    "#     g = sns.clustermap(-np.log10(val.astype(object).pivot(index=\"index\", columns=\"variable\",values=\"value\").fillna(1)))\n",
    "#     plt.suptitle(ind)\n",
    "#     g.ax_cbar.set(title=f\"-log10 p-val;cutoff is {-np.log10(p_thresh)}\")\n",
    "#     # g.ax_cbar.set(title=\"-log10 p-value\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ## Save significant results_df\n",
    "\n",
    "# out_df = results_df.rename({\"value\":\"p_value shuffle\", \"index\":\"clone\", \"variable\":\"lineage\"}, axis=1)\n",
    "# out_df = out_df[out_df[\"p_value shuffle\"]<p_thresh]\n",
    "\n",
    "# init_p_df = init_bh_enrichment.reset_index().melt(id_vars=[\"index\"]).rename({\"variable\":\"lineage\", \"index\":\"clone\",\n",
    "#                                                                           \"value\": \"BH_p_adj\"},axis=1).set_index([\"clone\", \"lineage\"])\n",
    "# init_p_df\n",
    "# #pd.merge(init_p_df, out_df, on=[\"clone\", \"lineage\"], how=\"inner\" )\n",
    "\n",
    "# out_df[\"BH_p_adj\"] = out_df.apply(lambda x:init_p_df.loc[(x[\"clone\"],x[\"lineage\"]), \"BH_p_adj\"], axis=1)\n",
    "# out_df\n",
    "\n",
    "# out_df.sort_values([\"method\", \"p_value shuffle\", \"BH_p_adj\"])\n",
    "\n",
    "# out_df[out_df[\"BH_p_adj\"] != 1]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
