{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding specific variants in each cluster\n",
    "Using the clones, we want to find variants in each clone that are specific to them.\n",
    "To define specificity, we will use two parameters: VAF frequency cutoff, and different %of population to have that variant.\n",
    "These parameters will be used to compare each clone's variants to the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from deap import base, algorithms\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import matplotlib\n",
    "matplotlib.rcParams['ps.useafm'] = True\n",
    "matplotlib.rcParams['pdf.use14corefonts'] = True\n",
    "#matplotlib.rcParams['text.usetex'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T20:12:12.105680Z",
     "start_time": "2021-05-13T20:12:12.099105Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    indir = snakemake.params.indir\n",
    "    outdir = snakemake.params.outdir\n",
    "    donor =  snakemake.params.donor\n",
    "    anno_cells_meta_f = snakemake.input.anno_cells_meta_f  #\"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/gff_A2_black/annotation_clones/se_cells_meta_labels.tsv\"\n",
    "    # Objective weights. order of the columns\n",
    "    weights =  snakemake.params.weights\n",
    "    objectives_l = snakemake.params.get(objectives_l, \n",
    "                                        [\"variants_with_clone_norm_by_1_over_nclones_with_variant\", \n",
    "                                         \"max_clone_ncells_over_nclones\", \"max_clone_ncells_over_ncells\", \n",
    "                                         \"pct_thresh\",\"other_pct_thresh\", \n",
    "                                         \"n_vars\", \"obj_nclones_more_than_one_unique\"])\n",
    "    ncpus = snakemake.params.get(ncpus, 8)\n",
    "    topn = snakemake.params.get(topn, 16)\n",
    "except:                                       \n",
    "    indir = \"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/\"\n",
    "    outdir = \"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/distinct_variants/donor0/scrap/\"\n",
    "    donor = 0\n",
    "    anno_cells_meta_f = \"/data/Mito_Trace/output/pipeline/v02/CHIP_b1/MTBlacklist_A2/data/merged/MT/cellr_True/numread_200/filters/minC10_minR50_topN0_hetT0.001_hetC10_hetCount5_bq20/mgatk/vireoIn/clones/variants_init/knn/kparam_30/gff_A2_black/annotation_clones/se_cells_meta_labels.tsv\"\n",
    "    # pct_thresh = [0.01, 0.1, 0.25, 0.4, 0.5, 0.75, 0.95]\n",
    "    # other_pct_thresh = [0.01, 0.1, 0.25, 0.5]\n",
    "    # af_thresh = [0, 0.01, 0.1, 0.25, 0.4]\n",
    "\n",
    "    # Objective weights. order of the columns\n",
    "    weights = [1,0,0,1,-1, 1, 1] #[1,1,1,1,1] #np.ones([len(objectives),])\n",
    "    objectives_l = [\"variants_with_clone_norm_by_1_over_nclones_with_variant\", \n",
    "                    \"max_clone_ncells_over_nclones\", \"max_clone_ncells_over_ncells\", \n",
    "                    \"pct_thresh\",\"other_pct_thresh\", \"n_vars\", \"obj_nclones_more_than_one_unique\"] #\"nvars\"\n",
    "    ncpus=8\n",
    "    topn=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T20:12:13.155526Z",
     "start_time": "2021-05-13T20:12:12.130665Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join, exists, dirname\n",
    "from glob import glob\n",
    "import pickle\n",
    "import mplh.cluster_help as ch\n",
    "import mplh.fig_utils as fu\n",
    "\n",
    "import os\n",
    "import vireoSNP\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.io import mmread\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hypergeom\n",
    "print(vireoSNP.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mplh import cluster_help as ch\n",
    "from vireoSNP import Vireo\n",
    "np.set_printoptions(formatter={'float': lambda x: format(x, '.3f')})\n",
    "\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = {ind:x for ind,x in enumerate(objectives_l)}\n",
    "weights = np.array(weights)\n",
    "\n",
    "assert(len(weights)==len(objectives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [\"pct_thresh\",\"af_thresh\", \"other_pct_thresh\"]\n",
    "n_params = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & preprocess:\n",
    "- AF df\n",
    "- DP df\n",
    "- cells_meta with clone labels. need to create name as donor_lineage\n",
    "\n",
    "Remove donor variants (>0.9 in 90% of pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "af_indir = join(indir, \"sc_af\", f\"donor{donor}\")\n",
    "\n",
    "AF_df = pd.read_csv(join(af_indir, \"af.tsv\"), index_col=0, sep=\"\\t\")\n",
    "DP_df = pd.read_csv(join(af_indir, \"dp.tsv\"), index_col=0, sep=\"\\t\")\n",
    "\n",
    "print(AF_df.shape)\n",
    "print(DP_df.shape)\n",
    "print(\"Depth\")\n",
    "print(DP_df.head())\n",
    "AF_df.head()\n",
    "\n",
    "cells_meta = pd.read_csv(join(indir, \"cells_meta.tsv\"), sep='\\t', index_col=\"ID\")#.sort_values([\"donor\", \"lineage\"])\n",
    "cells_meta[\"name\"] = cells_meta[\"donor\"].astype(str)+\"_\"+cells_meta[\"lineage\"].astype(str)\n",
    "# if \"donor_index\" in cells_meta.columns and \"lineage_index\" in cells_meta.columns:\n",
    "#     cells_meta = cells_meta.sort_values([\"donor_index\", \"lineage_index\"])\n",
    "#AD_df = pd.merge(AD_df, vcf[[\"#CHROM\", \"POS\", \"ALT\"]], how=\"inner\", left_index=True,right_index=True).set_index([\"#CHROM\", \"POS\", \"ALT\"])\n",
    "curr_labels = cells_meta[cells_meta[\"donor\"]==donor]\n",
    "curr_labels\n",
    "\n",
    "conditions = curr_labels[\"condition\"].unique()\n",
    "conditions\n",
    "\n",
    "def rm_high(df, thresh, pct_thresh):\n",
    "    return df.loc[~(((df>thresh).sum(axis=1)>pct_thresh*df.shape[0]))]\n",
    "\n",
    "def rm_low(df, thresh, pct_thresh):\n",
    "    return df.loc[~((df<thresh).sum(axis=1)>(pct_thresh*df.shape[1]))]\n",
    "\n",
    "    #return df.loc[~(((df<=thresh).sum(axis=1)>pct_thresh*df.shape[0]))]\n",
    "#df[(df<0.01).sum(axis=1)]\n",
    "\n",
    "## Get donor inds\n",
    "\n",
    "donor_inds = AF_df.index[((AF_df>0.9).sum(axis=1)>(0.9*AF_df.shape[1]))]\n",
    "donor_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all cells in each clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objective: \n",
    "1. Maximize: /sum_{v \\in V}{max({c_{ncells}\\in C}/|C|), where V is number of variants, c_{i,v,ncells} is number of cells in clone i with variant v, and C is the set of clones with the variant. We want to maximize this objective. Across all variants.\n",
    "2. Same as 1, but denominator is not number of clones with variant but number of cells with variant\n",
    "3. Maximize pct_thresh\n",
    "4. Minimize other_pct_thresh (not high priority)\n",
    "\n",
    "## Constraint:\n",
    "1. pct_thresh>=other_pct_thresh\n",
    "2. af_thresh*coverage_thresh>=2\n",
    "\n",
    "## Bounds:\n",
    "1. pct_thresh: 0.1-1\n",
    "2. other_pct_thresh: 0.1-1\n",
    "3. af_thresh: 0.01-0.4\n",
    "4. coverage_thresh: 2-60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "ic.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_variants(cln_af, other_af, pct_thresh, af_thresh, other_pct_thresh):\n",
    "    \"\"\" gets the distinct variants in a clone.\n",
    "    \"\"\"\n",
    "    n_thresh = pct_thresh*cln_af.shape[1]\n",
    "    n_oth_thresh = other_pct_thresh*other_af.shape[1]\n",
    "    bin_cln = cln_af>af_thresh\n",
    "    bin_other = other_af>af_thresh\n",
    "    cells_above = bin_cln.sum(axis=1)\n",
    "    pct_cells_above = cells_above/bin_cln.shape[1]\n",
    "    up_vars = bin_cln.loc[cells_above > n_thresh].index\n",
    "    cells_other_above = bin_other.sum(axis=1)\n",
    "    pct_cells_other_above = cells_other_above/bin_other.shape[1]\n",
    "    up_oth_vars = bin_other.loc[cells_other_above > n_oth_thresh].index\n",
    "    uniq_vars = list(set(up_vars) - set(up_oth_vars))\n",
    "    out = pd.DataFrame(index=uniq_vars, data={\"n_cells\":cells_above.loc[uniq_vars].values, \n",
    "                                              \"n_other_cells\": cells_other_above.loc[uniq_vars].values,\n",
    "                                              \"pct_above\": pct_cells_above,\n",
    "                                              \"pct_other_above\": pct_cells_other_above})\n",
    "    out[\"pct_thresh\"] = pct_thresh\n",
    "    out[\"af_thresh\"] = af_thresh\n",
    "    out[\"other_pct_thresh\"] = other_pct_thresh\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_clones_unique_variants(solution, data):\n",
    "    all_unique_df = []\n",
    "    pct_thresh, af_thresh, other_pct_thresh = solution[\"pct_thresh\"], solution[\"af_thresh\"], solution[\"other_pct_thresh\"] #solution[0], solution[1], solution[2]\n",
    "    curr_labels = data[\"curr_labels\"]\n",
    "    AF_df = data[\"AF_df\"]\n",
    "    DP_df = data[\"DP_df\"]\n",
    "    for cln, val in curr_labels.groupby(\"name\"):\n",
    "        ic(cln)\n",
    "        cln_af = AF_df.loc[:, val.index]\n",
    "        other_af = AF_df.loc[:, curr_labels.drop(val.index).index]\n",
    "        curr_dp = DP_df.loc[:, val.index]\n",
    "        curr_labs = curr_labels[curr_labels.index.isin(cln_af.columns)]\n",
    "        ic(cln_af.shape)\n",
    "        unique_df = get_unique_variants(cln_af, other_af, pct_thresh, af_thresh, other_pct_thresh)\n",
    "        unique_df[\"clone\"] = cln\n",
    "        unique_df[\"id\"] = unique_df[\"clone\"] + \"_\" + unique_df[\"pct_thresh\"].astype(str)+ \"_\" + unique_df[\"af_thresh\"].astype(str)+ \"_\" + unique_df[\"other_pct_thresh\"].astype(str)\n",
    "        unique_df[\"variant\"] = unique_df.index\n",
    "        unique_df = unique_df.set_index(\"id\")\n",
    "        all_unique_df.append(unique_df)\n",
    "    all_unique_df = pd.concat(all_unique_df)\n",
    "    all_unique_df[\"log2_n_cells\"] = np.log2(all_unique_df[\"n_cells\"]+1)\n",
    "    return all_unique_df\n",
    "\n",
    "\n",
    "\n",
    "def _objective_two_unique_vars_in_clone(all_unique_df, to_pivot=True):\n",
    "    if to_pivot:\n",
    "        if \"id\" in all_unique_df.columns:\n",
    "            df = all_unique_df.pivot(index=\"id\", columns=\"variant\", values=\"n_cells\").fillna(0).astype(int)\n",
    "        else:\n",
    "            df = all_unique_df.reset_index().pivot(index=\"id\", columns=\"variant\", values=\"n_cells\").fillna(0).astype(int)\n",
    "    else:\n",
    "        df = all_unique_df\n",
    "    vars_to_keep = df.loc[:,(df>0).sum()==1].columns # Variants with just 1 clone\n",
    "    clones_to_keep = df.loc[df.sum(axis=1)>1].index # Clones w >2 enriched variants\n",
    "    obj = 0\n",
    "    def cl_more_than_one(cl_ser):  \n",
    "        curr = cl_ser[cl_ser > 0] # variants in a clone\n",
    "        # check if more than one unique variant for this clone\n",
    "        return sum([True if x in vars_to_keep else False for x in curr.index]) > 1\n",
    "\n",
    "#     def cl_more_than_one(cl_ser, vars_to_keep):  \n",
    "#         curr = cl_ser[cl_ser > 0] # variants in a clone\n",
    "#         # check if more than one unique variant for this clone\n",
    "#         return sum([True if x in vars_to_keep else False for x in curr.index]) > 1\n",
    "    #obj = sum(df.loc[clones_to_keep].apply(lambda x: x(lambda y):, axis=0))\n",
    "    obj = sum(df.loc[clones_to_keep].apply(cl_more_than_one, axis=1))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _objectives(data):\n",
    "    all_unique_df = data[\"all_unique_df\"]\n",
    "    #print('all_unique_df', all_unique_df.head)\n",
    "    ic('all_unique_df', all_unique_df.shape)\n",
    "    obj_max_nce_over_ncl = 0\n",
    "    obj_max_nce_over_nce = 0\n",
    "    obj_cl_over_ncl = 0\n",
    "    obj_nvars = 0\n",
    "    if len(all_unique_df) == 0:\n",
    "        #print('all 0', all_unique_df.columns)\n",
    "        return {x:(-1*np.inf) for x in objectives_l} # return score of 0 since all positive values\n",
    "    obj_d = all_unique_df.iloc[0][\"pct_thresh\"] \n",
    "    obj_e = all_unique_df.iloc[0][\"other_pct_thresh\"]\n",
    "    for v, v_df in all_unique_df.groupby(\"variant\"):\n",
    "        ic(v)\n",
    "        max_ncells = max(v_df[\"n_cells\"])\n",
    "        n_clones = len(set(v_df[\"clone\"].values))\n",
    "        obj_max_nce_over_ncl += max_ncells/n_clones\n",
    "        obj_max_nce_over_nce += max_ncells/v_df[\"n_cells\"].sum()\n",
    "        \n",
    "        if n_clones != 0:\n",
    "            obj_cl_over_ncl += 1/n_clones\n",
    "            obj_nvars += 1\n",
    "        \n",
    "    # calculate objective number of clones with more than one unique variant\n",
    "    obj_nclones_more_than_one_unique =  _objective_two_unique_vars_in_clone(all_unique_df, to_pivot=True)\n",
    "    \n",
    "    objectives = {\"variants_with_clone_norm_by_1_over_nclones_with_variant\":obj_cl_over_ncl,\n",
    "                  \"max_clone_ncells_over_nclones\":obj_max_nce_over_ncl, \n",
    "                  \"max_clone_ncells_over_ncells\":obj_max_nce_over_nce, \n",
    "                  \"pct_thresh\":obj_d,\"other_pct_thresh\":obj_e,\n",
    "                   \"n_vars\":obj_nvars, \"obj_nclones_more_than_one_unique\": obj_nclones_more_than_one_unique}\n",
    "    return objectives\n",
    "\n",
    "def _constraints(solution):\n",
    "    #if solution[\"pct_thresh\"] < solution[\"other_pct_thresh\"]:\n",
    "    if \"coverage_thresh\" not in solution:\n",
    "        return None\n",
    "    if solution[\"af_thresh\"]*solution[\"coverage_thresh\"] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_series(individual_ser, AF_df, DP_df, curr_labels, return_data=False):\n",
    "    params = individual_ser.to_dict()\n",
    "    #print('params', params)\n",
    "    #solution = {\"pct_thresh\": individual[0], \"af_thresh\":individual[1],  \"other_pct_thresh\": individual[2]}\n",
    "    data = {\"AF_df\": AF_df, \"DP_df\":DP_df, \"curr_labels\":curr_labels} \n",
    "    all_unique_df = get_clones_unique_variants(params, data)\n",
    "    data[\"all_unique_df\"] = all_unique_df\n",
    "    eval_out = _objectives(data)\n",
    "    if return_data:\n",
    "        return pd.Series(eval_out), data\n",
    "    else:\n",
    "        return pd.Series(eval_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA or gridsearch on parameters and evaluate objectives \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parallel setup\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=ncpus, progress_bar=True)\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pct_thresh = np.arange(0.05, 1, 0.05)\n",
    "other_pct_thresh = np.arange(0.005, 1, 0.05)\n",
    "af_thresh = np.arange(0.005, 1, 0.05)\n",
    "\n",
    "\n",
    "# There are 7 params to use for calling the clone\n",
    "params = {\"pct_thresh\": pct_thresh,\n",
    "          \"other_pct_thresh\": other_pct_thresh,\n",
    "          \"af_thresh\": af_thresh,}\n",
    "full_params = list(product(*list(params.values())))\n",
    "full_params = pd.DataFrame(full_params, columns=params.keys())\n",
    "\n",
    "print(full_params.shape)\n",
    "full_params.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if full_params.shape[0]>=10000:\n",
    "    results_df = full_params.sample(10000).parallel_apply(evaluate_series, args=(AF_df, DP_df, curr_labels), axis=1)\n",
    "else:\n",
    "    results_df = full_params.parallel_apply(evaluate_series, args=(AF_df, DP_df, curr_labels), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the 0 scores before weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(results_df>0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_inds = results_df.loc[(results_df==0).all(axis=1)].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_multi_rank(results, weights):\n",
    "    if \"multi\" in results.columns: #in case multi was added before\n",
    "        rank_results = results.drop(\"multi\",axis=1).rank(na_option='top')\n",
    "    else:\n",
    "        rank_results = results.rank(na_option='top')\n",
    "    rank_results[\"multi\"] = (weights*rank_results).sum(axis=1)\n",
    "    return rank_results.sort_values(by=\"multi\")[::-1]\n",
    "\n",
    "def set_multi(results, weights):\n",
    "    print(results.shape)\n",
    "    # first normalize results for each column to sum to 1\n",
    "    objs_total = results.replace([-np.inf, np.inf], np.nan).sum(axis=0)\n",
    "    print('objs_total', objs_total.head())\n",
    "    results_norm = results.apply(lambda x: x/objs_total.loc[x.name], axis=0)\n",
    "    \n",
    "    results_norm[\"multi\"] = (weights*results_norm).sum(axis=1)\n",
    "    return results_norm.sort_values(by=\"multi\")[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.replace([-np.inf, np.inf], np.nan).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_norm = set_multi(results_df, weights)\n",
    "\n",
    "rank_df = set_multi_rank(results_norm, weights)\n",
    "rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.replace([-np.inf], np.nan).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop the na ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_results = results_norm.loc[results_norm[\"multi\"].isnull()]\n",
    "results_norm = results_norm.loc[~(results_norm[\"multi\"].isnull())]\n",
    "results_norm\n",
    "#results_norm.loc[results_norm[\"multi\"] == np.nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(results_norm[\"multi\"])\n",
    "plt.title(\"multiobjective function (want to maximize)\")\n",
    "plt.savefig(join(outdir, \"loss_multi.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(rank_df[\"multi\"])\n",
    "#plt.title(\"multiobjective function rankings (ties are averaged, want to maximize)\")\n",
    "#plt.savefig(join(outdir, \"multi_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(rank_df[\"variants_with_clone_norm_by_1_over_nclones_with_variant\"])\n",
    "plt.title(\"objective: variants_with_clone_norm_by_1_over_nclones_with_variant (want to maximize)\")\n",
    "plt.savefig(join(outdir, \"loss_variants_with_clone_norm_by_1_over_nclones_with_variant.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(rank_df[\"obj_nclones_more_than_one_unique\"])\n",
    "plt.title(\"objective: Number of clones with at least 2 unique variants (want to maximize)\")\n",
    "plt.savefig(join(outdir, \"loss_variants_with_clone_norm_by_1_over_nclones_with_variant.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_results(results_df, rank_df, n=12):\n",
    "    filt_rank = rank_df.sort_values(by=[\"multi\"])[::-1].iloc[:n]\n",
    "    filt_results = results_df.loc[filt_rank.index]\n",
    "    return filt_rank, filt_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filt_rank, filt_results = get_top_n_results(results_norm, rank_df, n=topn)\n",
    "\n",
    "filt_results.columns = [f\"{x}_obj\" for x in filt_results.columns]\n",
    "\n",
    "filt_results = pd.merge(filt_results, full_params, left_index=True, right_index=True, how=\"left\")\n",
    "filt_rank = filt_rank.loc[filt_results.index]\n",
    "filt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df = []\n",
    "all_objs = {}\n",
    "for ind, val in filt_results.iterrows():\n",
    "    print(ind)\n",
    "    obj_out, data = evaluate_series(val, AF_df, DP_df, curr_labels, return_data=True)\n",
    "    all_df.append(data[\"all_unique_df\"])\n",
    "    all_objs[ind] = obj_out \n",
    "all_df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = all_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Either re-run results for top to get unique_df for each param, or store in a dict (harder b/c of the parallel apply). After, need to make all_df by concatenating the results.\n",
    "Plot A: heatmap FacetGrid, but each axis is a combination of the parameters (can I do col=[\"colA\", \"colC\"]?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap_input = all_df[[\"n_cells\", \"variant\"]].reset_index().pivot(index=\"id\", columns=\"variant\", values=\"n_cells\").fillna(0).astype(int)\n",
    "meta_df = all_df[[\"af_thresh\", \"other_pct_thresh\", \"pct_thresh\", \"clone\"]]\n",
    "meta_df = meta_df.loc[~(meta_df.index.duplicated())]\n",
    "meta_df = meta_df.sort_values([\"af_thresh\",\"pct_thresh\", \"other_pct_thresh\", \"clone\"])\n",
    "heatmap_input = heatmap_input.loc[meta_df.index]\n",
    "\n",
    "# Get the variants based on total number of cells across parameters\n",
    "heatmap_input = heatmap_input.loc[:,heatmap_input.sum().sort_values()[::-1].index]\n",
    "variants_order = heatmap_input.columns\n",
    "\n",
    "# Get the clones based on total number of cells across parameters\n",
    "def clone_sum(val):\n",
    "    #print(val)\n",
    "    return(heatmap_input.loc[val.index].sum())\n",
    "\n",
    "clone_sums = meta_df.groupby(\"clone\").apply(clone_sum)\n",
    "clone_sums = clone_sums.loc[:, clone_sums.sum().sort_values()[::-1].index]\n",
    "clones_order = clone_sums.index\n",
    "clones_order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create column with all parameter names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_str(ser, param_names):\n",
    "    name = \"\"\n",
    "    for p in param_names:\n",
    "        name = name + f\"{p}={ser[p]:.3f};    \"\n",
    "    return name\n",
    "\n",
    "def params_and_multi_str(ser):\n",
    "    param_str = ser[\"params\"]\n",
    "    name = f\"params:\\n{param_str.strip()}\\nObjective score={ser['multi_obj']} (want to maximize)\" \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df[\"params\"] = all_df.apply(params_to_str, axis=1, args=(param_names,))\n",
    "all_df.head()\n",
    "\n",
    "filt_results[\"params\"] = filt_results.apply(params_to_str, axis=1, args=(param_names,))\n",
    "filt_results\n",
    "\n",
    "\n",
    "filt_results[\"params_multi\"] = filt_results.apply(params_and_multi_str, axis=1)\n",
    "filt_results\n",
    "tmp = filt_results.set_index(\"params\")\n",
    "all_df[\"multi_obj\"] = all_df.apply(lambda x: tmp.loc[x[\"params\"], \"multi_obj\"], axis=1)\n",
    "del tmp                               \n",
    "\n",
    "all_df[\"params_multi\"] = all_df.apply(params_and_multi_str, axis=1)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp = heatmap_input.loc[heatmap_input.index.str.contains(\"0.8_0.005_0.055\")]\n",
    "# tmp.loc[\"0_19_0.8_0.005_0.055\", \"14905A\"] = 1\n",
    "# tmp.loc[\"0_33_0.8_0.005_0.055\", \"14233G\"] = 1\n",
    "# #print('tmp', tmp.head())\n",
    "\n",
    "# tmp_obj = _objective_two_unique_vars_in_clone(tmp, to_pivot=False)\n",
    "# tmp_obj\n",
    "\n",
    "#     for cl in clones_to_keep:\n",
    "#         curr_v = df.loc[[cl], (df.loc[cl]>0)]\n",
    "#         # if clones with >2 variants, count how many of those are unique\n",
    "#         n_unique_vars = sum(curr_v.apply(lambda x: x.name in vars_to_keep, axis=0))\n",
    "#         if n_unique_vars>1:\n",
    "#             obj += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of clone-variant number of cells for all the distinguishing variants of the top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(*args, **kwargs):\n",
    "    data = kwargs.pop('data')\n",
    "    #param_names = kwargs.pop('param_names', None)\n",
    "    print(data.shape)\n",
    "    d = data.pivot(index=args[1], columns=args[0], values=args[2]).fillna(0)\n",
    "    \n",
    "    # get all clones and variants\n",
    "    d_full = pd.DataFrame(index=clones_order, columns=variants_order)\n",
    "    d_full.loc[:,:] = 0\n",
    "    d_full.loc[d.index,d.columns] = d\n",
    "    d_full = d_full.astype(float)\n",
    "    \n",
    "    # get cluster results with jaccard\n",
    "    g = sns.clustermap(d_full, method=\"single\")\n",
    "    inds = g.dendrogram_row.dendrogram[\"leaves\"]\n",
    "    cols = g.dendrogram_col.dendrogram[\"leaves\"]\n",
    "    plt.close(g.fig)  \n",
    "\n",
    "    sns.heatmap(d_full.iloc[inds,cols], cbar_kws = dict(orientation=\"vertical\"), **kwargs)\n",
    "    plt.title(data[\"params_multi\"].values[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"params_multi\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=all_df.reset_index(), height=4, sharey=False, sharex=False,\n",
    "                   col=\"params\", col_wrap=4, col_order=filt_results[\"params\"].values, margin_titles=True)\n",
    "\n",
    "fg.map_dataframe(draw_heatmap, 'variant','clone', 'log2_n_cells')#, cbar=False)\n",
    "#fg.set_titles(row_template = 'other_pct_thresh: {row_name}', col_template = 'pct_thresh: {col_name}')\n",
    "fg.fig.suptitle(f\"Best parameter combinations shown in order\")\n",
    "fg.fig.subplots_adjust(top=0.9, hspace = 0.8)\n",
    "\n",
    "plt.title(\"multiobjective function (want to maximize)\")\n",
    "plt.savefig(join(outdir, \"top_param_results.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table for the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_params = filt_results.iloc[0]\n",
    "best_params = pd.DataFrame(best_params).transpose()\n",
    "best_params.index = [\"objective_scores\"]\n",
    "best_params.loc[\"weight\"] = None\n",
    "for obj, w in zip(objectives_l, weights):\n",
    "    best_params.loc[\"weight\", f\"{obj}_obj\"] = w\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df = all_df[all_df['params'] == best_params.loc[\"objective_scores\", \"params\"]]\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clone_var_table = (out_df.pivot(index= 'variant',columns='clone', values='log2_n_cells').fillna(0))\n",
    "clone_var_table\n",
    "\n",
    "clones_keep = clone_var_table.loc[:, ~((clone_var_table==0).all(axis=0))].columns\n",
    "vars_keep = clone_var_table.loc[~((clone_var_table==0).all(axis=1))].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.clustermap(clone_var_table)\n",
    "plt.title(best_params.loc[\"objective_scores\", \"params_multi\"])\n",
    "plt.savefig(join(outdir, \"best_params.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(clone_var_table.loc[vars_keep,clones_keep])\n",
    "plt.title(best_params.loc[\"objective_scores\", \"params_multi\"])\n",
    "plt.savefig(join(outdir, \"best_params_filt.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clone-variant table and the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_var_table.to_csv(join(outdir, \"best_params_clone_vars.csv\"))\n",
    "clone_var_table.loc[vars_keep,clones_keep].to_csv(join(outdir, \"best_params_filt_clone_vars.csv\"))\n",
    "best_params.to_csv(join(outdir, \"best_params.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_results.to_csv(join(outdir, \"param_results.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter cells meta and af and df and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filt_curr_labels = curr_labels[curr_labels[\"name\"].isin(clones_keep)]\n",
    "anno_cells = pd.read_csv(anno_cells_meta_f, sep=\"\\t\", index_col=0)\n",
    "#anno_cells = anno_cells.loc[anno_cells[\"ID\"].isin(filt_curr_labels.index)]\n",
    "#out_cells_meta = anno_cells.loc[anno_cells[\"ID\"].isin(filt_curr_labels.index)]\n",
    "\n",
    "# overlap cells of anno and curr labels\n",
    "cells_to_keep = set(anno_cells[\"ID\"].values).intersection(set(filt_curr_labels.index))\n",
    "out_cells_meta = anno_cells.loc[anno_cells[\"ID\"].isin(cells_to_keep)]\n",
    "out_cells_meta =  out_cells_meta.reset_index().set_index(\"ID\")\n",
    "\n",
    "# out_AF_df = AF_df.loc[vars_keep, out_cells_meta.index]\n",
    "# out_DP_df = DP_df.loc[vars_keep, out_cells_meta.index]\n",
    "out_AF_df = AF_df.loc[vars_keep, out_cells_meta.index]# [\"ID\"]]\n",
    "out_DP_df = DP_df.loc[vars_keep, out_cells_meta.index]#[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(out_cells_meta.shape)\n",
    "print(out_AF_df.shape)\n",
    "print(out_DP_df.shape)\n",
    "\n",
    "assert((out_AF_df.index==out_DP_df.index).all())\n",
    "assert((out_AF_df.columns==out_DP_df.columns).all())\n",
    "assert((out_AF_df.columns==out_cells_meta.index).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save cells-meta, af and dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cells_meta.to_csv(join(outdir, \"cells_meta.tsv\"),sep=\"\\t\")\n",
    "out_AF_df.to_csv(join(outdir, \"af.tsv\"), sep=\"\\t\")\n",
    "out_DP_df.to_csv(join(outdir, \"dp.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_AF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.clustermap(clone_var_table.loc[vars_keep,clones_keep])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5560.364131,
   "end_time": "2021-01-27T10:03:31.625455",
   "environment_variables": {},
   "exception": null,
   "input_path": "/data2/mito_lineage/src/vireo/vireoSNP_clones.ipynb",
   "output_path": "results/jan21_2021/chrM/P2_cellSNP_minC200_minAF0.01/lineage_chrM.ipynb",
   "parameters": {
    "AD_F": "data/jan21_2021/chrM/P2_cellSNP_minC200_minAF0.01/cellSNP.tag.AD.mtx",
    "DP_F": "data/jan21_2021/chrM/P2_cellSNP_minC200_minAF0.01/cellSNP.tag.DP.mtx"
   },
   "start_time": "2021-01-27T08:30:51.261324",
   "version": "2.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
